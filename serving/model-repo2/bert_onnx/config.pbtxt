# Triton config for BERT-large ONNX (Hugging Face Optimum export).
# Model: google-bert/bert-large-uncased (336M params).
# Inputs: input_ids, attention_mask, token_type_ids (dynamic sequence length).
# Output: last_hidden_state [batch, seq_len, 1024].

name: "bert_onnx"
platform: "onnxruntime_onnx"
max_batch_size: 32

dynamic_batching {
  preferred_batch_size: [ 1 ]
  max_queue_delay_microseconds: 0
}

input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ -1 ]
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [ -1 ]
  },
  {
    name: "token_type_ids"
    data_type: TYPE_INT64
    dims: [ -1 ]
  }
]
output [
  {
    name: "last_hidden_state"
    data_type: TYPE_FP32
    dims: [ -1, 1024 ]
  }
]
