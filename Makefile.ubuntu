# Fake model server: start / stop via Make (Ubuntu/Linux)
# Usage: make -f Makefile.ubuntu start [PORT=8000]  |  make -f Makefile.ubuntu stop  |  make -f Makefile.ubuntu restart  |  make -f Makefile.ubuntu status

PORT ?= 8000
PIDFILE := .server.pid
SHELL := /bin/bash
.SHELLFLAGS := -ec

.PHONY: start stop restart status workload workload-schedule loadgen-workload kserve-schedule infer-curl

start:
	@if [ -f '$(PIDFILE)' ]; then \
		p=$$(cat '$(PIDFILE)'); \
		if kill -0 $$p 2>/dev/null; then \
			echo 'Server already running (PID' $$p '). Use make stop first.'; exit 1; \
		fi; \
		rm -f '$(PIDFILE)'; \
	fi; \
	cd fakeserver && uv run python run.py --host 127.0.0.1 --port $(PORT) & \
	echo $$! > '../$(PIDFILE)'; \
	cd ..; \
	echo 'Server started (PID' $$(cat '$(PIDFILE)') '). http://127.0.0.1:$(PORT)'; \
	sleep 1; \
	if [ -f '$(PIDFILE)' ]; then \
		p=$$(cat '$(PIDFILE)'); \
		if kill -0 $$p 2>/dev/null; then \
			echo 'Server running (PID' $$p '). Port $(PORT).'; \
			curl -s -o /dev/null -w '' http://127.0.0.1:$(PORT)/health/ 2>/dev/null || true; \
		else \
			echo 'PID file exists but process not running.'; \
		fi; \
	else \
		echo 'Server not running.'; \
	fi

stop:
	@if [ -f '$(PIDFILE)' ]; then \
		p=$$(cat '$(PIDFILE)'); \
		if kill -0 $$p 2>/dev/null; then \
			kill $$p 2>/dev/null || kill -9 $$p 2>/dev/null; \
			echo 'Server stopped (PID' $$p ').'; \
		else \
			echo 'Process' $$p 'not running.'; \
		fi; \
		rm -f '$(PIDFILE)'; \
	else \
		echo 'No $(PIDFILE) found. Server may not be running.'; \
	fi

restart: stop
	@sleep 1; $(MAKE) -f Makefile.ubuntu start

status:
	@if [ -f '$(PIDFILE)' ]; then \
		p=$$(cat '$(PIDFILE)'); \
		if kill -0 $$p 2>/dev/null; then \
			echo 'Server running (PID' $$p '). Port $(PORT).'; \
			curl -s http://127.0.0.1:$(PORT)/health/ 2>/dev/null || true; \
		else \
			echo 'PID file exists but process not running. Run make stop to clean.'; \
		fi; \
	else \
		echo 'Server not running (no $(PIDFILE)).'; \
	fi

workload:
	uv run python custom/workload.py --url http://127.0.0.1:$(PORT) --duration 10 --concurrency 6 --mix 1,1,1

workload-schedule:
	uv run python custom/loadgen_workload.py --url http://127.0.0.1:$(PORT) --schedule "0-5:3,5-10:10,10-15:20" --workload-log custom/workload_schedule.csv

loadgen-workload:
	uv run python custom/loadgen_workload.py --url http://127.0.0.1:$(PORT) --duration-ms 10000 --target-qps 10 --mix 1,1,1 --workload-log custom/workload.csv

KSERVE_URL ?= http://140.113.194.247:8080
kserve-schedule:
	uv run python custom/loadgen_workload.py --kserve --url $(KSERVE_URL) --schedule "0-5:3,5-10:10,10-15:20,15-25:40" --workload-log custom/workload_kserve_schedule.csv

infer-curl:
	curl -sS -X POST http://140.113.194.247:8080/v2/models/mnist/infer -H "Content-Type: application/json" -H "Host: mlflow-v2-wine-classifier-predictor.default.example.com" --data-binary @infer_v2.json
