[project]
name = "mlperf-inference"
version = "0.1.0"
description = "MLPerf inference (fakeserver + custom workload use root .venv)"
requires-python = ">=3.9"
dependencies = [
    "flask>=3.0.0",
    "requests>=2.28.0",
    "tritonclient[grpc]",
    "mlcommons-loadgen>=6.0.0",
    "matplotlib>=3.5.0",
    "grpcio>=1.60.0",
    "grpcio-tools>=1.60.0",
    "numpy>=1.24.0",
]

[project.optional-dependencies]
yolo-serving = [
    "mlc-scripts",
    "ultralytics",
    "opencv-python-headless",
]
bert-export = [
    "optimum[onnx]",
    "transformers",
    "torch",
    "onnx",
    "onnxruntime>=1.16,<1.24",
]
sdxl-export = [
    "optimum[onnxruntime]",
    "diffusers",
    "transformers",
    "torch",
    "onnx",
    "onnxruntime>=1.16,<1.24",
]
